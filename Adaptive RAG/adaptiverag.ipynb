{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73e38000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\\n\",\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27f2f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\\n\",\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "### from langchain_cohere import CohereEmbeddings\n",
    "# Set embeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "    ]\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "# Add to vectorstore\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da4fb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\"\n",
    "    )\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "    Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(\n",
    "        question_router.invoke(\n",
    "            {\"question\": \"Who won the Cricket world cup 2023 \"}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2c272c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05ce2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call (Requires importing ChatOpenAI and ChatPromptTemplate)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# NOTE: The 'retriever' and 'docs' variables are assumed to be defined elsewhere for this to run.\n",
    "# For example purposes, we will mock them.\n",
    "\n",
    "# Mocking the retriever and docs for a complete example\n",
    "class MockDocument:\n",
    "    def __init__(self, page_content):\n",
    "        self.page_content = page_content\n",
    "\n",
    "class MockRetriever:\n",
    "    def invoke(self, question):\n",
    "        # A mock list of documents\n",
    "        return [\n",
    "            MockDocument(\"This is the first document about tool use.\"),\n",
    "            MockDocument(\"This document is about agent memory and its importance in long-term tasks.\"),\n",
    "            MockDocument(\"A third unrelated document on large language models.\"),\n",
    "        ]\n",
    "\n",
    "retriever = MockRetriever()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content # Grabbing the second document for grading\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain definition\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# Execution\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "309fe2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the rag-prompt is to guide a language model to answer questions using only the provided context. It also instructs the model to acknowledge when the answer is not available in that context.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI # Added missing import for ChatOpenAI\n",
    "\n",
    "# -----------------\n",
    "# Prompt & LLM Setup\n",
    "# -----------------\n",
    "# Pull a standard RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Initialize the Language Model (LLM)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# -----------------\n",
    "# Document Formatting\n",
    "# -----------------\n",
    "def format_docs(docs):\n",
    "    \"\"\"Formats a list of documents into a single string separated by newlines.\"\"\"\n",
    "    # Corrected indentation for the function body\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# -----------------\n",
    "# RAG Chain Definition\n",
    "# -----------------\n",
    "# NOTE: The 'docs' variable in the run step is expected to be a list of Document objects.\n",
    "# The 'prompt' expects 'context' and 'question' as inputs.\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# -----------------\n",
    "# Execution Example (Requires 'docs' and 'question' to be defined)\n",
    "# -----------------\n",
    "\n",
    "# Mocking the required variables for a complete, runnable example:\n",
    "class MockDocument:\n",
    "    def __init__(self, page_content):\n",
    "        self.page_content = page_content\n",
    "\n",
    "question = \"What is the purpose of the rag-prompt?\"\n",
    "docs = [\n",
    "    MockDocument(\"The rlm/rag-prompt is designed to guide an LLM to answer a question based *only* on the provided context.\"),\n",
    "    MockDocument(\"It typically includes instructions to admit when the answer is not available in the context.\"),\n",
    "]\n",
    "\n",
    "# The prompt template expects the context to be a string. \n",
    "# We should format the docs before invoking the chain if the prompt expects a string context,\n",
    "# but the standard rlm/rag-prompt handles the list of documents directly.\n",
    "# However, if your prompt expects a *string* context (which is common), \n",
    "# you would chain the formatting function first, like this:\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# rag_chain = (\n",
    "#     {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "#     | prompt \n",
    "#     | llm \n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# For simplicity and to match the original structure, we'll keep the direct invoke but \n",
    "# acknowledge the standard LangChain way often uses the Runnable Passthrough pattern.\n",
    "\n",
    "# Let's use the simplest version that matches your original code's final call:\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question}) \n",
    "# NOTE: I am using format_docs() here because most generic RAG prompts expect a single \n",
    "# string for the 'context' variable, not a list of Document objects.\n",
    "\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fe927e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# NOTE: 'docs' and 'generation' are assumed to be variables defined earlier.\n",
    "\n",
    "# -----------------\n",
    "# Data Model\n",
    "# -----------------\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# -----------------\n",
    "# LLM Setup\n",
    "# -----------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# -----------------\n",
    "# Prompt Definition\n",
    "# -----------------\n",
    "system = (\n",
    "    \"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\n",
    ")\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Hallucination Grader Chain\n",
    "# -----------------\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "# -----------------\n",
    "# Execution Example (Requires 'docs' and 'generation' to be defined)\n",
    "# -----------------\n",
    "# MOCKING variables for a complete, runnable example:\n",
    "docs = \"Fact 1: The sun is a star. Fact 2: Stars produce their own light.\"\n",
    "generation = \"The sun is a star that generates light.\" \n",
    "\n",
    "result = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a28f82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# NOTE: 'question' and 'generation' are assumed to be variables defined earlier.\n",
    "\n",
    "# -----------------\n",
    "# Data Model\n",
    "# -----------------\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess whether the answer addresses the question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# -----------------\n",
    "# LLM Setup\n",
    "# -----------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# -----------------\n",
    "# Prompt Definition\n",
    "# -----------------\n",
    "system = (\n",
    "    \"You're a grader assessing whether an answer addresses/resolves a question. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no'. 'Yes' means the answer resolves the question.\"\n",
    ")\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Answer Grader Chain\n",
    "# -----------------\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "\n",
    "# -----------------\n",
    "# Execution Example\n",
    "# -----------------\n",
    "# Define example variables for demonstration:\n",
    "question = \"When was Python invented?\"\n",
    "generation = \"Python was invented in the late 1980s by Guido van Rossum.\"\n",
    "\n",
    "# Invoke the grader chain\n",
    "result = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9b82b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What's the stuff about the brain and computers?\n",
      "Rewritten Question: What is the relationship between brain function and computer technology?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser # Added missing import\n",
    "# NOTE: 'question' is assumed to be a variable defined earlier.\n",
    "\n",
    "# -----------------\n",
    "# LLM Setup\n",
    "# -----------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# -----------------\n",
    "# Prompt Definition\n",
    "# -----------------\n",
    "system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version \"\n",
    "    \"that is optimized for vectorstore retrieval. Look at the input and try to reason \"\n",
    "    \"about the underlying semantic intent / meaning.\"\n",
    ")\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Question Re-writer Chain\n",
    "# -----------------\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "# -----------------\n",
    "# Execution Example\n",
    "# -----------------\n",
    "# Define example variable for demonstration:\n",
    "question = \"What's the stuff about the brain and computers?\"\n",
    "\n",
    "# Invoke the re-writer chain\n",
    "result = question_rewriter.invoke({\"question\": question})\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Rewritten Question: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71817121",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97215f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's initial question.\n",
    "        generation: The LLM's generated answer.\n",
    "        documents: A list of retrieved documents (or strings).\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f58982d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from pprint import pprint\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# =================================================================\n",
    "# 1. State Definition\n",
    "# =================================================================\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "\n",
    "# =================================================================\n",
    "# 2. Mock Dependencies (CRITICAL FIX APPLIED HERE)\n",
    "# =================================================================\n",
    "\n",
    "class MockGraderResult:\n",
    "    def __init__(self, score):\n",
    "        self.binary_score = score\n",
    "\n",
    "class MockRouterResult:\n",
    "    def __init__(self, datasource):\n",
    "        self.datasource = datasource\n",
    "\n",
    "class MockComponent:\n",
    "    def __init__(self, return_value=None):\n",
    "        self.return_value = return_value\n",
    "\n",
    "    def invoke(self, input):\n",
    "        # Determine the calling instance to ensure correct return type\n",
    "        \n",
    "        # 1. QUESTION ROUTER (returns MockRouterResult)\n",
    "        if self is question_router:\n",
    "            if input.get('question') == \"route_to_web\":\n",
    "                return MockRouterResult(\"web_search\")\n",
    "            return MockRouterResult(\"vectorstore\") # Default RAG route\n",
    "\n",
    "        # 2. RETRIEVER (returns List[Document])\n",
    "        if self is retriever:\n",
    "            # FIX: Always return a List[Document]\n",
    "            return [\n",
    "                Document(page_content=f\"ML Document 1: Machine learning is the study of computer algorithms.\"),\n",
    "                Document(page_content=\"Irrelevant Document 2: This is about cats.\")\n",
    "            ]\n",
    "        \n",
    "        # 3. QUESTION REWRITER (returns str)\n",
    "        if self is question_rewriter:\n",
    "            return f\"Rephrased question: {input.get('question')}\"\n",
    "\n",
    "        # 4. GRADER & CHAIN MOCKS (based on input structure)\n",
    "        if isinstance(input, dict):\n",
    "            # Web Search Tool (returns list of dicts)\n",
    "            if 'query' in input:\n",
    "                return [{'content': f\"Web search result for: {input['query']}\"}]\n",
    "            \n",
    "            # RAG Chain (returns str)\n",
    "            if 'context' in input:\n",
    "                return \"The answer based on context is provided.\"\n",
    "            \n",
    "            # Retrieval Grader (returns MockGraderResult)\n",
    "            if 'document' in input: \n",
    "                # Ensure the grader can score the mock documents successfully\n",
    "                if \"machine learning\" in input['document'].lower():\n",
    "                    return MockGraderResult(\"yes\")\n",
    "                return MockGraderResult(\"no\")\n",
    "                \n",
    "            # Hallucination/Answer Graders (returns MockGraderResult)\n",
    "            if 'generation' in input:\n",
    "                return MockGraderResult(\"yes\") # Always 'yes' for simple mock\n",
    "\n",
    "        return self.return_value # Fallback\n",
    "\n",
    "# Mock instances (must be defined AFTER MockComponent)\n",
    "# The identity of these instances is CRUCIAL for the fixed logic above.\n",
    "retriever = MockComponent()\n",
    "rag_chain = MockComponent()\n",
    "retrieval_grader = MockComponent()\n",
    "question_rewriter = MockComponent()\n",
    "web_search_tool = MockComponent()\n",
    "question_router = MockComponent()\n",
    "hallucination_grader = MockComponent()\n",
    "answer_grader = MockComponent()\n",
    "\n",
    "# =================================================================\n",
    "# 3. Node Functions (Logic is Correct)\n",
    "# =================================================================\n",
    "\n",
    "def retrieve(state: GraphState) -> dict:\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    # The fixed mock ensures this returns List[Document]\n",
    "    documents = retriever.invoke({\"question\": question}) \n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state: GraphState) -> dict:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state: GraphState) -> dict:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents: # This loop is now safe\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "def transform_query(state: GraphState) -> dict:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state: GraphState) -> dict:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results_doc = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": [web_results_doc]}\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 4. Conditional Edges (Routers) (Logic is Correct)\n",
    "# =================================================================\n",
    "\n",
    "def route_question(state: GraphState) -> str:\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    source = question_router.invoke({\"question\": state[\"question\"]})\n",
    "\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    return \"retrieve\" # The default route\n",
    "\n",
    "\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    \n",
    "    # Simpler logic for mock purposes: assume grounded, check if useful\n",
    "    score = answer_grader.invoke({\"question\": state[\"question\"], \"generation\": state[\"generation\"]})\n",
    "    \n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED AND ADDRESSES QUESTION---\")\n",
    "        return \"useful\"\n",
    "    \n",
    "    pprint(\"---DECISION: GENERATION IS NOT USEFUL, RE-TRY---\")\n",
    "    return \"not useful\"\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 5. Graph Setup (Logic is Correct)\n",
    "# =================================================================\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\", # Re-generate (based on hallucination)\n",
    "        \"useful\": END,               # End\n",
    "        \"not useful\": \"transform_query\", # Re-write query (based on answer failure)\n",
    "    },\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cddeb6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED AND ADDRESSES QUESTION---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is machine learning',\n",
       " 'generation': 'The answer based on context is provided.',\n",
       " 'documents': [Document(metadata={}, page_content='ML Document 1: Machine learning is the study of computer algorithms.')]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"question\":\"What is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4adbac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED AND ADDRESSES QUESTION---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is agent memory',\n",
       " 'generation': 'The answer based on context is provided.',\n",
       " 'documents': [Document(metadata={}, page_content='ML Document 1: Machine learning is the study of computer algorithms.')]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"question\":\"What is agent memory\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
