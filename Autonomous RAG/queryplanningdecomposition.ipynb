{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9d9d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader,WebBaseLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea421d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb2b2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Embed Documents\\n\",\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\n",
    "    ]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(WebBaseLoader(url).load())\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ca8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Schema\\n\",\n",
    "class RAGState(BaseModel):\n",
    "    question: str \n",
    "    sub_questions: List[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93890297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\\n\",\n",
    "## a. Query Planner: splits input question\n",
    "def plan_query(state: RAGState) -> RAGState:\n",
    "    prompt = f\"\"\"\n",
    "    Break the following complex question into 2-3 sub-questions:\n",
    "    Question: {state.question}\n",
    "    Sub-questions:\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    sub_questions = [line.strip(\"- \") for line in result.content.splitlines() if line.strip()]\n",
    "    return RAGState(question=state.question, sub_questions=sub_questions)\n",
    "## b. Retrieve documents for each sub-question\\n\",\n",
    "def retrieve_for_each(state: RAGState) -> RAGState:\n",
    "    all_docs = []\n",
    "    for sub in state.sub_questions:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=all_docs)\n",
    "## c. Generate final answer\n",
    "def generate_final_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    Use the context below to answer the question.\\n\",\n",
    "    Context:\n",
    "    {context}\n",
    "    Question: {state.question}\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=state.retrieved_docs, answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dfaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LangGraph\n",
    "builder = StateGraph(RAGState)\n",
    "builder.add_node(\"planner\", plan_query)\n",
    "builder.add_node(\"retriever\", retrieve_for_each)\n",
    "builder.add_node(\"responder\", generate_final_answer)\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae45a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Explain how agent loops work and what are the challenges in diffusion video generation?', 'sub_questions': ['1. What is an agent loop, and how does it function in the context of computational systems or artificial intelligence?', '2. What are the primary challenges faced in the process of diffusion video generation, and how do these challenges affect the overall quality and efficiency of the generated videos?'], 'retrieved_docs': [Document(id='0505d5e5-3a79-48ac-bdba-8aedefdfa87f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.'), Document(id='e02f53ca-be06-4cab-824f-8d4d2b91b00c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:'), Document(id='75a66cba-98ce-491d-b057-1fcc531e9629', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:'), Document(id='31efe30b-4ea7-48c3-96b9-a76cea5578b1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Tool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='f82f8d33-bb90-48b8-bfa1-b37464b84761', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\n🥑 Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en'}, page_content='Adapting Image Models to Generate Videos\\n\\nFine-tuning on Video Data\\n\\nTraining-Free Adaptation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:'), Document(id='a56af143-5917-4a60-a7ce-057b784d06eb', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\n🥑 Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en'}, page_content='In the case of video generation, we need the diffusion model to run multiple steps of upsampling for extending video length or increasing the frame rate. This requires the capability of sampling a second video $\\\\mathbf{x}^b$ conditioned on the first $\\\\mathbf{x}^a$, $\\\\mathbf{x}^b \\\\sim p_\\\\theta(\\\\mathbf{x}^b \\\\vert \\\\mathbf{x}^a)$, where $\\\\mathbf{x}^b$ might be an autoregressive extension of $\\\\mathbf{x}^a$ or be the missing frames in-between for a video $\\\\mathbf{x}^a$ at a low frame rate.'), Document(id='bff299ad-632f-48ac-9591-2e48e901a600', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\n🥑 Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 “Structure and Content-Guided Video Synthesis with Diffusion Models.”\\n[13] Bar-Tal et al. 2024 “Lumiere: A Space-Time Diffusion Model for Video Generation.”'), Document(id='ac9479de-40fc-4d31-b67d-0c492d9c3f9f', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\n🥑 Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en'}, page_content='Imagen Video also applies progressive distillation to speed up sampling and each distillation iteration can reduce the required sampling steps by half. Their experiments were able to distill all 7 video diffusion models down to just 8 sampling steps per model without any noticeable loss in perceptual quality.')], 'answer': \"**Agent Loops:**\\n\\nIn the context of LLM-powered autonomous agent systems, agent loops involve a series of actions and processes that enable the agent to function effectively based on past experiences and interactions. Here's how they typically work:\\n\\n1. **Memory**: Agents maintain a record of past experiences and interactions, allowing them to learn from history and improve future behaviors.\\n\\n2. **Planning**: Agents strategize and set goals based on current tasks and objectives. This may involve breaking complex tasks into smaller, manageable steps.\\n\\n3. **Reflection**: Agents review and evaluate their actions and decisions to improve their performance over time. By reflecting on past actions, they can identify successful strategies and areas for improvement.\\n\\n4. **Tool Use**: Agents can interact with external systems, utilizing APIs and databases to gather information not stored in the model’s weights. For instance, they might access the internet for updated information, execute code, or use other specific tools to complete tasks effectively.\\n\\n5. **Interaction with Other Agents**: Agents can also communicate and collaborate with other agents to achieve shared goals or solve complex problems.\\n\\n**Challenges in Diffusion Video Generation:**\\n\\nDiffusion models, known for their success in image synthesis, face unique challenges when extended to video generation. Here are some of the key challenges:\\n\\n1. **Increased Complexity**: Video generation is inherently more complex than image generation. Videos consist of multiple frames, requiring the model to maintain temporal coherence and continuity between frames.\\n\\n2. **High Computational Demand**: The generation of videos involves multiple steps of upsampling and interpolation. This requires significant computational resources to ensure smooth frame transitions and high-quality outputs.\\n\\n3. **Conditional Sampling**: The model must be able to sample a second video segment conditioned on a previous segment (e.g., filling in missing frames or extending an existing video). This adds a layer of complexity as the model needs to predict and generate coherent sequences over time.\\n\\n4. **Frame Rate and Length**: Ensuring that videos have an appropriate frame rate and length while maintaining quality is challenging. Diffusion models must adapt to various upsampling requirements based on the video's characteristics.\\n\\n5. **Autoregressive Extensions**: The model might need to perform autoregressive extensions where it predicts subsequent frames based on prior ones, maintaining consistency and alignment in the generated content over a duration.\\n\\nOverall, while agent loops enhance the capabilities of autonomous systems, diffusion models need to tackle significant challenges to seamlessly transition from static image generation to dynamic video creation.\"}\n",
      "🔍 Sub-questions:\n",
      "- 1. What is an agent loop, and how does it function in the context of computational systems or artificial intelligence?\n",
      "- 2. What are the primary challenges faced in the process of diffusion video generation, and how do these challenges affect the overall quality and efficiency of the generated videos?\n",
      "✅ Final Answer: **Agent Loops:**\n",
      "\n",
      "In the context of LLM-powered autonomous agent systems, agent loops involve a series of actions and processes that enable the agent to function effectively based on past experiences and interactions. Here's how they typically work:\n",
      "\n",
      "1. **Memory**: Agents maintain a record of past experiences and interactions, allowing them to learn from history and improve future behaviors.\n",
      "\n",
      "2. **Planning**: Agents strategize and set goals based on current tasks and objectives. This may involve breaking complex tasks into smaller, manageable steps.\n",
      "\n",
      "3. **Reflection**: Agents review and evaluate their actions and decisions to improve their performance over time. By reflecting on past actions, they can identify successful strategies and areas for improvement.\n",
      "\n",
      "4. **Tool Use**: Agents can interact with external systems, utilizing APIs and databases to gather information not stored in the model’s weights. For instance, they might access the internet for updated information, execute code, or use other specific tools to complete tasks effectively.\n",
      "\n",
      "5. **Interaction with Other Agents**: Agents can also communicate and collaborate with other agents to achieve shared goals or solve complex problems.\n",
      "\n",
      "**Challenges in Diffusion Video Generation:**\n",
      "\n",
      "Diffusion models, known for their success in image synthesis, face unique challenges when extended to video generation. Here are some of the key challenges:\n",
      "\n",
      "1. **Increased Complexity**: Video generation is inherently more complex than image generation. Videos consist of multiple frames, requiring the model to maintain temporal coherence and continuity between frames.\n",
      "\n",
      "2. **High Computational Demand**: The generation of videos involves multiple steps of upsampling and interpolation. This requires significant computational resources to ensure smooth frame transitions and high-quality outputs.\n",
      "\n",
      "3. **Conditional Sampling**: The model must be able to sample a second video segment conditioned on a previous segment (e.g., filling in missing frames or extending an existing video). This adds a layer of complexity as the model needs to predict and generate coherent sequences over time.\n",
      "\n",
      "4. **Frame Rate and Length**: Ensuring that videos have an appropriate frame rate and length while maintaining quality is challenging. Diffusion models must adapt to various upsampling requirements based on the video's characteristics.\n",
      "\n",
      "5. **Autoregressive Extensions**: The model might need to perform autoregressive extensions where it predicts subsequent frames based on prior ones, maintaining consistency and alignment in the generated content over a duration.\n",
      "\n",
      "Overall, while agent loops enhance the capabilities of autonomous systems, diffusion models need to tackle significant challenges to seamlessly transition from static image generation to dynamic video creation.\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"Explain how agent loops work and what are the challenges in diffusion video generation?\"\n",
    "    initial_state = RAGState(question=user_query)\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    print(final_state)\n",
    "    print(\"🔍 Sub-questions:\")\n",
    "    for q in final_state['sub_questions']:\n",
    "          print(\"-\", q)\n",
    "    print(\"✅ Final Answer:\", final_state['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
