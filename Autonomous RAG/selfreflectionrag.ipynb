{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f654cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7a165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47966bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"sample_docs.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8740c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Definition\n",
    "class RAGReflectionState(BaseModel):\n",
    "        question: str\n",
    "        retrieved_docs: List[Document] = []\n",
    "        answer: str = \"\"\n",
    "        reflection: str = \"\"\n",
    "        revised: bool = False\n",
    "        attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de150bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Nodes\n",
    "\n",
    "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
    "        docs = retriever.invoke(state.question)\n",
    "        return state.model_copy(update={\"retrieved_docs\": docs})\n",
    "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    context = \"\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to answer the question:\\n\",\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {state.question}\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375ab0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Reflect\n",
    "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    prompt = f\"\"\"\n",
    "    Reflect on the following answer to see if it fully addresses the question.\n",
    "    State YES if it is complete and correct, or NO with an explanation.\n",
    "    Question: {state.question}\n",
    "    Answer: {state.answer}\n",
    "    Respond like:\n",
    "    Reflection: YES or NO\n",
    "    Explanation: ...\"\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50693cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizer\n",
    "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd2feab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph DAG\\n\",\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "builder.set_entry_point(\"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
    "    )\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e13bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Final Answer: The question appears to be about transformer variants used in production deployments, but the provided context doesn't directly address this. However, common transformer variants that are often used in production for various applications include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Widely used for tasks like natural language understanding and question answering.\n",
      "\n",
      "2. **GPT (Generative Pre-trained Transformer)**: Used in tasks requiring text generation, such as chatbots and conversational AI.\n",
      "\n",
      "3. **Transformers for Vision (e.g., Vision Transformer, ViT)**: Used in image classification and other computer vision tasks.\n",
      "\n",
      "4. **T5 (Text-to-Text Transfer Transformer)**: Designed to handle a variety of NLP tasks through a unified text-to-text framework.\n",
      "\n",
      "5. **DistilBERT**: A lighter and faster version of BERT, optimized for performance and efficiency while retaining much of BERT's accuracy.\n",
      "\n",
      "These transformer models have been adapted and fine-tuned for a wide range of specific applications in production across different industries.\n",
      "nüîÅ Reflection Log: Reflection: YES  \n",
      "Explanation: The answer adequately addresses the question by listing several transformer model variants commonly deployed in production settings. It provides a brief description of each variant and highlights their primary application areas, which aligns with the inquiry about transformer variants used in practical, real-world deployments.\n",
      "üîÑ Total Attempts: 1\n"
     ]
    }
   ],
   "source": [
    "#Run the Agent\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "    print(\"üß† Final Answer:\", result[\"answer\"])\n",
    "    print(\"nüîÅ Reflection Log:\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
