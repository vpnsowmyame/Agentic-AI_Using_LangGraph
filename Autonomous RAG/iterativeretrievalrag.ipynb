{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7203454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cdf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e376ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load And Embed Documents\\n\",\n",
    "docs = TextLoader(\"sample_docs.txt\", encoding=\"utf-8\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c1029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Agent State\n",
    "class IterativeRAGState(BaseModel):\n",
    "    question: str\n",
    "    refined_question: str = \"\"\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    verified: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33fc2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve Node\n",
    "def retrieve_docs(state: IterativeRAGState) -> IterativeRAGState:\n",
    "        query = state.refined_question or state.question\n",
    "        docs = retriever.invoke(query)\n",
    "        return state.model_copy(update={\"retrieved_docs\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d82e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reflect And Verify\\n\",\n",
    "def generate_answer(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    context = \"\".join(doc.page_content for doc in state.retrieved_docs)\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {state.question}\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt.strip()).content.strip()\n",
    "    return state.model_copy(update={\"answer\": response, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7bcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reflect on answer\\n\",\n",
    "def reflect_on_answer(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate whether the answer below is factually sufficient and complete.\n",
    "    Question: {state.question}\n",
    "    Answer: {state.answer}\n",
    "    Respond 'YES' if it's complete, otherwise 'NO' with feedback.\n",
    "    \"\"\"\n",
    "    feedback = llm.invoke(prompt).content.lower()\n",
    "    verified = \"yes\" in feedback\n",
    "    return state.model_copy(update={\"verified\": verified})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67fe7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refine query\\n\",\n",
    "def refine_query(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    prompt = f\"\"\"\n",
    "    The answer appears incomplete. Suggest a better version of the query that would help retrieve more relevant context.\n",
    "    Original Question: {state.question}\n",
    "    Current Answer: {state.answer}\n",
    "    \"\"\"\n",
    "    new_query = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"refined_question\": new_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d985c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(IterativeRAGState)\n",
    "builder.add_node(\"retrieve\", retrieve_docs)\n",
    "builder.add_node(\"answer\", generate_answer)\n",
    "builder.add_node(\"reflect\", reflect_on_answer)\n",
    "builder.add_node(\"refine\", refine_query)\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"answer\")\n",
    "builder.add_edge(\"answer\", \"reflect\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "       lambda s: END if s.verified or s.attempts >= 2 else \"refine\"\n",
    ")\n",
    "builder.add_edge(\"refine\", \"retrieve\")\n",
    "builder.add_edge(\"answer\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"agent loops  and transformer-based systems?\"\n",
    "initial_state = IterativeRAGState(question=query)\n",
    "final = graph.invoke(initial_state)\n",
    "print(\"‚úÖ Final Answer:\", final[\"answer\"])\n",
    "print(\"üß† Verified:\", final[\"verified\"])\n",
    "print(\"üîÅ Attempts:\", final[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
