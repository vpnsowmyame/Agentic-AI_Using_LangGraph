{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1415836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E3E637AA50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E3E637B4D0>, root_client=<openai.OpenAI object at 0x000001E3E63781A0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E3E637B230>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751b98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Cache={}\n",
    "import time\n",
    "\n",
    "# Assuming Model_Cache (a dictionary) and llm (the language model) are defined in the scope\n",
    "\n",
    "def cache_model(query):\n",
    "    \"\"\"\n",
    "    Retrieves a response from Model_Cache if available (Cache Hit).\n",
    "    Otherwise, invokes the LLM, caches the result, and returns the response (Cache Miss).\n",
    "    The execution time printed is for the overall function call.\n",
    "    \"\"\"\n",
    "    start_time = time.time() # Start timing for the entire function call\n",
    "    \n",
    "    if Model_Cache.get(query):\n",
    "        print(\"**CACHE HIT**\")\n",
    "        response = Model_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS – EXECUTING MODEL***\")\n",
    "        \n",
    "        # NOTE: Using the original 'start_time' from the function entry ensures \n",
    "        # the entire LLM execution time is measured, including LangChain overhead.\n",
    "        # Original logic: 'start_time = time.time()' was REDUNDANT and measured only\n",
    "        # the tiny time between this line and 'response = llm.invoke(query)'.\n",
    "        \n",
    "        response = llm.invoke(query)\n",
    "        Model_Cache[query] = response\n",
    "        \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"EXECUTION TIME: {elapsed_time:.2f} seconds\")\n",
    "    return response\n",
    "\n",
    "# Example initialization (required for the code above to run):\n",
    "# Model_Cache = {} \n",
    "# class MockLLM:\n",
    "#     def invoke(self, query):\n",
    "#         time.sleep(1)\n",
    "#         return f\"Response to {query}\"\n",
    "# llm = MockLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7d6f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 1.04 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CMqF7CAY5e0hhCDnKGlRpeQSJnPRZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2f7d493-ccef-4ea6-8ca9-5285addc311a-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab2a098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CMqF7CAY5e0hhCDnKGlRpeQSJnPRZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2f7d493-ccef-4ea6-8ca9-5285addc311a-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a720c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CACHE HIT**\n",
      "EXECUTION TIME: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CMqF7CAY5e0hhCDnKGlRpeQSJnPRZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2f7d493-ccef-4ea6-8ca9-5285addc311a-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f6c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 14.66 seconds\n",
      "content='LangGraph is a cutting-edge concept that embodies the intersection of natural language processing (NLP) and graph theory, aimed at improving the way machines understand and generate human language. As language continues to evolve and grow in complexity, researchers and developers are increasingly turning to sophisticated models that can represent the nuances of language structure and meaning. LangGraph is at the forefront of this revolution, providing a framework that utilizes graph structures to capture relationships between words, phrases, and larger linguistic constructs.\\n\\nAt the core of LangGraph is the idea of representing language as a graph where nodes represent linguistic elements such as words or phrases, and edges capture the relationships or dependencies between these elements. This graph structure allows for a more nuanced understanding of language, allowing for the representation of semantic meaning, syntactic relationships, and even pragmatic context. By visualizing and analyzing language in this way, LangGraph can help overcome some of the limitations of traditional sequential approaches seen in standard NLP models, such as recurrent neural networks (RNNs) or transformers, which often struggle with long-range dependencies or context.\\n\\nThe development of LangGraph is particularly relevant in an era where the demand for more sophisticated AI applications is skyrocketing. From chatbots that need to respond meaningfully in conversation to systems that analyze vast amounts of text for insights, the ability to model complex relationships in language is crucial. This is where the graph-based approach of LangGraph truly shines, as it can represent intricate dependencies and relational dynamics that traditional models may miss. \\n\\nMoreover, LangGraph supports the integration of various sources of information, allowing for multi-modal understanding of language. For instance, it can incorporate not just text but also visual information, audio cues, and other contextual data into its graph structure. This multi-faceted approach ensures that the AI understands not just the words being spoken or written, but also the context in which they play out—potentially leading to more accurate and human-like interactions.\\n\\nOne of the exciting applications of LangGraph lies in enhancing machine learning models used for tasks like information extraction, sentiment analysis, and text summarization. By employing graph-based algorithms, these tasks can be performed with higher accuracy and efficiency. For instance, in sentiment analysis, LangGraph can identify not just the individual sentiments associated with specific words, but also how they interact and influence one another within the broader context of a sentence or paragraph.\\n\\nThe educational implications of LangGraph are also notable. By providing a visual representation of language and its complexities, it can serve as an effective tool for teaching language concepts and improving literacy. Students could engage with language in a new, interactive way, enhancing their comprehension and analytical skills.\\n\\nIn summary, LangGraph represents a significant advancement in the field of NLP, incorporating graph theory to capture the complexities of human language. Its ability to depict relationships and dependencies more effectively than traditional models has broad applications, from enhancing AI communication systems to supporting educational initiatives. As language technology continues to evolve, approaches like LangGraph will likely play a vital role in creating AI that understands, generates, and interacts with human language in ways that are increasingly sophisticated and meaningful.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 617, 'prompt_tokens': 18, 'total_tokens': 635, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CMqGRFS852RG1FNjIXDV6Q0gcM6dA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e2039e42-faf2-4bd7-8838-4878b87a7c5a-0' usage_metadata={'input_tokens': 18, 'output_tokens': 617, 'total_tokens': 635, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2783a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CACHE HIT**\n",
      "EXECUTION TIME: 0.00 seconds\n",
      "content='LangGraph is a cutting-edge concept that embodies the intersection of natural language processing (NLP) and graph theory, aimed at improving the way machines understand and generate human language. As language continues to evolve and grow in complexity, researchers and developers are increasingly turning to sophisticated models that can represent the nuances of language structure and meaning. LangGraph is at the forefront of this revolution, providing a framework that utilizes graph structures to capture relationships between words, phrases, and larger linguistic constructs.\\n\\nAt the core of LangGraph is the idea of representing language as a graph where nodes represent linguistic elements such as words or phrases, and edges capture the relationships or dependencies between these elements. This graph structure allows for a more nuanced understanding of language, allowing for the representation of semantic meaning, syntactic relationships, and even pragmatic context. By visualizing and analyzing language in this way, LangGraph can help overcome some of the limitations of traditional sequential approaches seen in standard NLP models, such as recurrent neural networks (RNNs) or transformers, which often struggle with long-range dependencies or context.\\n\\nThe development of LangGraph is particularly relevant in an era where the demand for more sophisticated AI applications is skyrocketing. From chatbots that need to respond meaningfully in conversation to systems that analyze vast amounts of text for insights, the ability to model complex relationships in language is crucial. This is where the graph-based approach of LangGraph truly shines, as it can represent intricate dependencies and relational dynamics that traditional models may miss. \\n\\nMoreover, LangGraph supports the integration of various sources of information, allowing for multi-modal understanding of language. For instance, it can incorporate not just text but also visual information, audio cues, and other contextual data into its graph structure. This multi-faceted approach ensures that the AI understands not just the words being spoken or written, but also the context in which they play out—potentially leading to more accurate and human-like interactions.\\n\\nOne of the exciting applications of LangGraph lies in enhancing machine learning models used for tasks like information extraction, sentiment analysis, and text summarization. By employing graph-based algorithms, these tasks can be performed with higher accuracy and efficiency. For instance, in sentiment analysis, LangGraph can identify not just the individual sentiments associated with specific words, but also how they interact and influence one another within the broader context of a sentence or paragraph.\\n\\nThe educational implications of LangGraph are also notable. By providing a visual representation of language and its complexities, it can serve as an effective tool for teaching language concepts and improving literacy. Students could engage with language in a new, interactive way, enhancing their comprehension and analytical skills.\\n\\nIn summary, LangGraph represents a significant advancement in the field of NLP, incorporating graph theory to capture the complexities of human language. Its ability to depict relationships and dependencies more effectively than traditional models has broad applications, from enhancing AI communication systems to supporting educational initiatives. As language technology continues to evolve, approaches like LangGraph will likely play a vital role in creating AI that understands, generates, and interacts with human language in ways that are increasingly sophisticated and meaningful.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 617, 'prompt_tokens': 18, 'total_tokens': 635, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CMqGRFS852RG1FNjIXDV6Q0gcM6dA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e2039e42-faf2-4bd7-8838-4878b87a7c5a-0' usage_metadata={'input_tokens': 18, 'output_tokens': 617, 'total_tokens': 635, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed3f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 13.84 seconds\n",
      "content='LangGraph is an innovative tool designed to enhance the capabilities of natural language processing (NLP) applications. It operates at the intersection of language understanding, machine learning, and graph theory, providing a unique framework for processing and generating human-like text. The concept of LangGraph stems from the recognition that language is not merely a collection of words but a complex system of relationships and meanings that can be effectively modeled using graphs.\\n\\nAt its core, LangGraph utilizes graph structures to represent language constructs. In this model, words, phrases, and sentences are nodes, while the relationships between them—such as syntactic connections, semantic associations, and contextual relevance—are represented as edges. This configuration allows LangGraph to visualize and manipulate language more dynamically than traditional sequential models that often fail to capture the intricate dependencies present in human communication.\\n\\nOne of the significant advantages of using graph-based representations in language processing is their capability to facilitate reasoning and inference. In a graph, multiple paths can exist between words or ideas, enabling the system to explore various relationships and derive meaning more holistically. For instance, if we consider the sentence, “The cat chased the mouse,” LangGraph can analyze various aspects, such as the entities involved (the cat and the mouse), the action (chased), and the context in which this action occurs. This holistic understanding enables a more nuanced generation of responses or analyses.\\n\\nFurthermore, LangGraph is particularly effective in handling multi-turn conversations where context retention is crucial. By maintaining a graph representation of previous exchanges, it can track contextual changes and reference past interactions, leading to more coherent and contextually appropriate responses. This stands in contrast to many linear models that may struggle to maintain context over extended dialogue.\\n\\nIn practical applications, LangGraph has found utility in several domains. In customer service, for instance, it can analyze customer inquiries and map their relationships to appropriate responses efficiently. In educational technology, it can create adaptive learning experiences by assessing students’ understanding and tailoring content dynamically based on their interactions. Moreover, in content creation, writers can use LangGraph to organize their thoughts and explore connections between ideas, thereby enhancing creativity and productivity.\\n\\nMoreover, the adaptability of LangGraph fosters an environment for advanced semantic analysis and sentiment detection. By examining the relationships between words within a larger context, it can discern subtleties in meaning and tone that may be overlooked by traditional NLP systems. This feature is especially valuable in applications such as social media monitoring, content moderation, and brand management, where understanding public sentiment is crucial.\\n\\nThe implementation of LangGraph is not without challenges. Building accurate and comprehensive graph representations requires sophisticated algorithms and significant computational resources. Additionally, ensuring that the graph remains up-to-date and reflective of the latest language use and trends demands ongoing maintenance and adaptation.\\n\\nIn summary, LangGraph stands as a promising development within the field of natural language processing, offering researchers and developers a powerful tool for representing and interacting with language. Through its graph-based approach, it provides a framework for understanding and generating human-like text that is more adaptable and context-aware than many of its predecessors. As the demand for advanced language technologies continues to grow, LangGraph may play a pivotal role in shaping the future of human-computer interaction and communication.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 641, 'prompt_tokens': 16, 'total_tokens': 657, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CMqHSnN4mTt7A9eC5rQLA70DNUIpy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d81e8418-f233-4779-923a-3242aa6764c9-0' usage_metadata={'input_tokens': 16, 'output_tokens': 641, 'total_tokens': 657, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "225c214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Advanced CAG\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import TypedDict, List, Optional\n",
    "import time\n",
    "\n",
    "# ---- LangGraph / LangChain ----\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# NOTE: The HuggingFaceEmbeddings import path is deprecated. \n",
    "# It should be imported from langchain_community.\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ---- FAISS vector stores ----\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b16d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= CONFIG =================\n",
    "# Removed the non-breaking space before the comment\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "VECTOR_DIM = 384\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "RETRIEVE_TOP_K = 4\n",
    "CACHE_TOP_K = 3\n",
    "\n",
    "CACHE_DISTANCE_THRESHOLD = 0.45\n",
    "\n",
    "# Optional TTL for cache entries (seconds). 0 = disabled.\n",
    "CACHE_TTL_SEC = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d089ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming TypedDict, List, Optional, and Document are imported\n",
    "# from typing, typing_extensions, and langchain_core.documents\n",
    "\n",
    "# ================= STATE ==================\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state passed between nodes in the RAG graph.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    normalized_question: str\n",
    "    context_docs: List[Document]\n",
    "    answer: Optional[str]\n",
    "    citations: List[str]\n",
    "    cache_hit: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aaac99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Win 11\\Desktop\\Agentic AI Workspace\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBED = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582ff71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming faiss, VECTOR_DIM, EMBED, FAISS, and InMemoryDocstore are imported/defined\n",
    "\n",
    "# ----- QA CACHE (EMPTY, SAFE INIT) -----\n",
    "# Distance metric: L2 (Euclidean distance); lower score means closer/more similar.\n",
    "qa_index = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "\n",
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=qa_index,\n",
    "    docstore=InMemoryDocstore({}), # Empty docstore\n",
    "    index_to_docstore_id={}       # Empty index map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e560ac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1e3cbd13620>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87473ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming FAISS and EMBED are imported/defined\n",
    "\n",
    "# ----- RAG STORE (demo only) -----\n",
    "RAG_STORE = FAISS.from_texts(\n",
    "    texts=[\n",
    "        \"LangGraph lets you compose stateful LLM workflows as graphs.\",\n",
    "        \"In LangGraph, nodes can be cached; node caching memoizes outputs keyed by inputs for a TTL.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) retrieves external context and injects it into prompts.\",\n",
    "        \"Semantic caching reuses prior answers when new questions are semantically similar.\",\n",
    "    ],\n",
    "    embedding=EMBED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57262347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E3CBDF91D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E3CBDF9590>, root_client=<openai.OpenAI object at 0x000001E3CBDF8F50>, root_async_client=<openai.AsyncOpenAI object at 0x000001E3CBDF9310>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)\n",
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "373f5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ NODES ===================\n",
    "# Assuming RAGState, Document, time, QA_CACHE, RAG_STORE, \n",
    "# RETRIEVE_TOP_K, CACHE_TOP_K, CACHE_TTL_SEC, and llm are defined in the scope\n",
    "\n",
    "def normalize_query(state: RAGState) -> RAGState:\n",
    "    \"\"\"Normalizes the user's question for easier comparison.\"\"\"\n",
    "    q = (state[\"question\"] or \"\").strip()\n",
    "    state[\"normalized_question\"] = q.lower()\n",
    "    return state\n",
    "\n",
    "def semantic_cache_lookup(state: RAGState) -> RAGState:\n",
    "    \"\"\"Checks the semantic cache (FAISS) for a similar pre-answered question.\"\"\"\n",
    "    q = state[\"normalized_question\"]\n",
    "    state[\"cache_hit\"] = False  # default\n",
    "\n",
    "    if not q:\n",
    "        return state\n",
    "\n",
    "    # Guard: FAISS crashes if index is empty\n",
    "    if getattr(QA_CACHE, \"index\", None) is None or QA_CACHE.index.ntotal == 0:\n",
    "        return state\n",
    "\n",
    "    # Search cache for similar questions\n",
    "    # Returns (Document, distance) where lower distance is better\n",
    "    hits = QA_CACHE.similarity_search_with_score(q, k=CACHE_TOP_K)\n",
    "    if not hits:\n",
    "        return state\n",
    "\n",
    "    best_doc, dist = hits[0]\n",
    "\n",
    "    # Optional TTL check\n",
    "    if CACHE_TTL_SEC > 0:\n",
    "        ts = best_doc.metadata.get(\"ts\")\n",
    "        if ts is None or (time.time() - float(ts)) > CACHE_TTL_SEC:\n",
    "            return state\n",
    "\n",
    "    # Distance threshold check (L2 distance: lower = more similar)\n",
    "    if dist <= CACHE_DISTANCE_THRESHOLD:\n",
    "        cached_answer = best_doc.metadata.get(\"answer\")\n",
    "        if cached_answer:\n",
    "            state[\"answer\"] = cached_answer\n",
    "            state[\"citations\"] = [\"(cache)\"]\n",
    "            state[\"cache_hit\"] = True\n",
    "\n",
    "    return state\n",
    "\n",
    "def respond_from_cache(state: RAGState) -> RAGState:\n",
    "    \"\"\"A placeholder node to explicitly route control flow when a cache hit occurs.\"\"\"\n",
    "    return state\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    \"\"\"Retrieves context documents from the main RAG store.\"\"\"\n",
    "    q = state[\"normalized_question\"]\n",
    "    docs = RAG_STORE.similarity_search(q, k=RETRIEVE_TOP_K)\n",
    "    state[\"context_docs\"] = docs\n",
    "    return state\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    \"\"\"Generates the final answer using retrieved context and the LLM.\"\"\"\n",
    "    q = state[\"question\"]\n",
    "    docs = state.get(\"context_docs\", [])\n",
    "    ctx = \"\\n\\n\".join([f\"[doc-{i}] {d.page_content}\" for i, d in enumerate(docs, start=1)])\n",
    "\n",
    "    system = (\n",
    "        \"You are a precise RAG assistant. Use the context when helpful. \"\n",
    "        \"Cite with [doc-i] markers if you use a fact from the context.\"\n",
    "    )\n",
    "    user = f\"Question: {q}\\n\\nContext:\\n{ctx}\\n\\nWrite a concise answer with citations.\"\n",
    "\n",
    "    # FIX: Corrected LLM variable name from LLM to the standard 'llm'\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ])\n",
    "    \n",
    "    state[\"answer\"] = resp.content\n",
    "    # NOTE: This citation logic assumes the LLM correctly cites. It generates \n",
    "    # a list of all possible citations based on the docs passed in.\n",
    "    state[\"citations\"] = [f\"[doc-{i}]\" for i in range(1, len(docs) + 1)]\n",
    "    return state\n",
    "\n",
    "def cache_write(state: RAGState) -> RAGState:\n",
    "    \"\"\"Writes the successful question/answer pair to the QA cache.\"\"\"\n",
    "    q = state[\"normalized_question\"]\n",
    "    a = state.get(\"answer\")\n",
    "    if not q or not a:\n",
    "        return state\n",
    "\n",
    "    QA_CACHE.add_texts(\n",
    "        texts=[q],\n",
    "        metadatas=[{\n",
    "            \"answer\": a,\n",
    "            \"ts\": time.time(),\n",
    "        }]\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0af46551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming RAGState, StateGraph, END, normalize_query, semantic_cache_lookup,\n",
    "# respond_from_cache, retrieve, generate, cache_write, and MemorySaver are defined.\n",
    "\n",
    "# ============== GRAPH WIRING ==============\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes (Name, Function)\n",
    "graph.add_node(\"normalize_query\", normalize_query)\n",
    "graph.add_node(\"semantic_cache_lookup\", semantic_cache_lookup)\n",
    "graph.add_node(\"respond_from_cache\", respond_from_cache)\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"cache_write\", cache_write)\n",
    "\n",
    "# Set the start of the workflow\n",
    "graph.set_entry_point(\"normalize_query\")\n",
    "\n",
    "# Edge from start to cache lookup\n",
    "graph.add_edge(\"normalize_query\", \"semantic_cache_lookup\")\n",
    "\n",
    "# Conditional logic (Router) for the cache\n",
    "def _branch(state: RAGState) -> str:\n",
    "    \"\"\"Decides if the workflow should use the cache or perform RAG.\"\"\"\n",
    "    return \"respond_from_cache\" if state.get(\"cache_hit\") else \"retrieve\"\n",
    "\n",
    "# Route based on cache hit/miss\n",
    "graph.add_conditional_edges(\n",
    "    \"semantic_cache_lookup\",\n",
    "    _branch,\n",
    "    {\n",
    "        \"respond_from_cache\": \"respond_from_cache\",\n",
    "        \"retrieve\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edges for cache hit path\n",
    "graph.add_edge(\"respond_from_cache\", END)\n",
    "\n",
    "# Edges for RAG path\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"cache_write\")\n",
    "\n",
    "# Edge to end after cache write\n",
    "graph.add_edge(\"cache_write\", END)\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "# app # (To display the compiled graph object in a notebook environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b762b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Q1: Cache Miss (Expected RAG) ---\n",
      "Answer: LangGraph is a framework that allows users to compose stateful workflows using large language models (LLMs) organized as graphs, facilitating the management and execution of complex tasks. One of its features includes caching nodes which memoizes outputs based on inputs for a defined time-to-live (TTL) [doc-1][doc-2].\n",
      "Citations: ['[doc-1]', '[doc-2]', '[doc-3]', '[doc-4]']\n",
      "Cache hit?: False\n"
     ]
    }
   ],
   "source": [
    "# ================= DEMO ===================\n",
    "# Assuming 'app' (the compiled graph) is defined, and the necessary RAGState keys \n",
    "# are being correctly initialized as dictionaries here, though the RAGState \n",
    "# definition requires a full state object.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration dictionary for LangGraph thread management\n",
    "    thread_cfg = {\"configurable\": {\"thread_id\": \"demo-user-1\"}}\n",
    "\n",
    "    # First Query (Expected Cache Miss and RAG execution)\n",
    "    q1 = \"What is LangGraph ?\"\n",
    "    \n",
    "    # Initialize state with mandatory keys for the RAGState TypedDict\n",
    "    initial_state = {\n",
    "        \"question\": q1,\n",
    "        \"normalized_question\": \"\",  # Will be set by normalize_query node\n",
    "        \"context_docs\": [],\n",
    "        \"answer\": None,\n",
    "        \"citations\": [],\n",
    "        \"cache_hit\": False,\n",
    "    }\n",
    "\n",
    "    # Invoke the graph\n",
    "    out1 = app.invoke(initial_state, thread_cfg)\n",
    "    \n",
    "    print(\"--- Q1: Cache Miss (Expected RAG) ---\")\n",
    "    print(\"Answer:\", out1[\"answer\"])\n",
    "    print(\"Citations:\", out1.get(\"citations\"))\n",
    "    print(\"Cache hit?:\", out1.get(\"cache_hit\"))\n",
    "\n",
    "# NOTE: For a complete demo, you would run a second, similar query here \n",
    "# to test the cache hit logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441bc0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph is a framework that allows users to compose stateful workflows using large language models (LLMs) organized as graphs, facilitating the management and execution of complex tasks. One of its features includes caching nodes which memoizes outputs based on inputs for a defined time-to-live (TTL) [doc-1][doc-2].\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'app' (the compiled graph), 'thread_cfg', and the necessary RAGState keys \n",
    "# are defined in the scope.\n",
    "\n",
    "q1 = \"Explain about LangGraph ?\"\n",
    "\n",
    "# Full state initialization ensures all keys expected by RAGState are present\n",
    "initial_state = {\n",
    "    \"question\": q1,\n",
    "    \"normalized_question\": \"\",\n",
    "    \"context_docs\": [],\n",
    "    \"answer\": None,\n",
    "    \"citations\": [],\n",
    "    \"cache_hit\": False,\n",
    "}\n",
    "\n",
    "# Invoke the graph with the initial state and thread config\n",
    "out1 = app.invoke(initial_state, thread_cfg)\n",
    "\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b17f54cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph agents are components within the LangGraph framework that facilitate the creation of stateful workflows using large language models (LLMs) organized in a graph structure. These agents can utilize caching mechanisms to improve performance: node caching allows outputs to be memoized based on inputs for a specific time-to-live (TTL) [doc-1], while semantic caching further enhances efficiency by reusing previous answers when new questions are semantically similar [doc-4]. Additionally, LangGraph supports Retrieval-Augmented Generation (RAG), which enhances the agent's capabilities by retrieving external context to enrich prompts [doc-3].\n",
      "Citations: ['[doc-1]', '[doc-2]', '[doc-3]', '[doc-4]']\n",
      "Cache hit?: False\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'app' (the compiled graph), 'thread_cfg', and the necessary RAGState keys \n",
    "# are defined in the scope.\n",
    "\n",
    "q1 = \"Explain about LangGraph agents ?\"\n",
    "\n",
    "# Full state initialization ensures all keys expected by RAGState are present\n",
    "initial_state = {\n",
    "    \"question\": q1,\n",
    "    \"normalized_question\": \"\",\n",
    "    \"context_docs\": [],\n",
    "    \"answer\": None,\n",
    "    \"citations\": [],\n",
    "    \"cache_hit\": False,\n",
    "}\n",
    "\n",
    "# Invoke the graph with the initial state and thread config\n",
    "out1 = app.invoke(initial_state, thread_cfg)\n",
    "\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c581d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph agents are components within the LangGraph framework that facilitate the creation of stateful workflows using large language models (LLMs) organized in a graph structure. These agents can utilize caching mechanisms to improve performance: node caching allows outputs to be memoized based on inputs for a specific time-to-live (TTL) [doc-1], while semantic caching further enhances efficiency by reusing previous answers when new questions are semantically similar [doc-4]. Additionally, LangGraph supports Retrieval-Augmented Generation (RAG), which enhances the agent's capabilities by retrieving external context to enrich prompts [doc-3].\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'app' (the compiled graph), 'thread_cfg', and the necessary imports \n",
    "# (like HumanMessage, RAGState, etc.) are defined in the scope.\n",
    "\n",
    "q1 = \"Explain about agents in Langgraph ?\"\n",
    "\n",
    "# Full state initialization ensures all keys expected by RAGState are present\n",
    "initial_state = {\n",
    "    \"question\": q1,\n",
    "    \"normalized_question\": \"\",\n",
    "    \"context_docs\": [],\n",
    "    \"answer\": None,\n",
    "    \"citations\": [],\n",
    "    \"cache_hit\": False,\n",
    "}\n",
    "\n",
    "# Invoke the graph with the initial state and thread config\n",
    "out1 = app.invoke(initial_state, thread_cfg)\n",
    "\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
